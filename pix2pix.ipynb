{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/2wscfy492xv8p55yn6b0rthw0000gn/T/ipykernel_39822/3500667393.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(conv.weight, 0, 0.02)\n",
      "/var/folders/3g/2wscfy492xv8p55yn6b0rthw0000gn/T/ipykernel_39822/3500667393.py:39: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(convT.weight, 0, 0.02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 64, 128, 128])\n",
      "torch.Size([2, 128, 64, 64])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 512, 16, 16])\n",
      "torch.Size([2, 512, 8, 8])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 512, 2, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from preprocess import GetDataset\n",
    "import random\n",
    "\n",
    "dataset = GetDataset()\n",
    "\n",
    "\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "def downsample(in_channels, out_channels, size, padding=1, apply_batchnorm=True):\n",
    "    #3 in_channels, 3 out_channels, 4x4 size, 2 stride\n",
    "    conv = torch.nn.Conv2d(in_channels, out_channels, size, stride=2, padding=padding, bias=False)\n",
    "    torch.nn.init.normal(conv.weight, 0, 0.02)\n",
    "\n",
    "    #expected 3 channels\n",
    "    batchnorm = torch.nn.BatchNorm2d(out_channels) \n",
    "    leakyrelu = torch.nn.LeakyReLU()\n",
    "    \n",
    "    def call(x):\n",
    "        print(x.shape)\n",
    "        x = conv(x)\n",
    "        if apply_batchnorm:\n",
    "            x = batchnorm(x)\n",
    "        x = leakyrelu(x)\n",
    "        return x\n",
    "\n",
    "    return call\n",
    "\n",
    "\n",
    "\n",
    "def upsample(in_channels, out_channels, size, apply_dropout=False):\n",
    "    #3 in_channels, 3 out_channels, 4x4 size, 2 stride\n",
    "    convT = torch.nn.ConvTranspose2d(in_channels, out_channels, size, stride=2, bias=False)\n",
    "    torch.nn.init.normal(convT.weight, 0, 0.02)\n",
    "\n",
    "    # No batchnorm if out_channel is of size 1\n",
    "    batchnorm = torch.nn.BatchNorm2d(out_channels) \n",
    "    relu = torch.nn.ReLU()\n",
    "    dropout = torch.nn.Dropout()\n",
    "\n",
    "    def call(x):\n",
    "        x = convT(x)\n",
    "        x = batchnorm(x)\n",
    "        if apply_dropout:\n",
    "            x = dropout(x)\n",
    "        x = relu(x)\n",
    "        return x\n",
    "    \n",
    "    return call\n",
    "\n",
    "#inputs go here\n",
    "\n",
    "# down_model = downsample(None)\n",
    "# up_model = upsample(None)\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # inputs = torch.tensor(np.zeros(3,256,256))\n",
    "        self.down_stack = [\n",
    "            downsample(3, 64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "            downsample(64, 128, 4),  # (batch_size, 64, 64, 128)\n",
    "            downsample(128, 256, 4),  # (batch_size, 32, 32, 256)\n",
    "            downsample(256, 512, 4),  # (batch_size, 16, 16, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 8, 8, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 4, 4, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 2, 2, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "        self.up_stack = [\n",
    "            upsample(512, 512, 2, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "            upsample(1024, 512, 2, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "            upsample(1024, 512, 2, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "            upsample(1024, 512, 2),  # (batch_size, 16, 16, 1024)\n",
    "            upsample(1024, 256, 2),  # (batch_size, 32, 32, 512)\n",
    "            upsample(512, 128, 2),  # (batch_size, 64, 64, 256)\n",
    "            upsample(256, 64, 2),  # (batch_size, 128, 128, 128)\n",
    "        ]\n",
    "\n",
    "        self.last = torch.nn.ConvTranspose2d(128, 3, 2, stride=2, bias=False)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    # torch.nn.init.normal(last.weight, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling through the model\n",
    "        skips = []\n",
    "        \n",
    "        for down in self.down_stack:\n",
    "            x = x.to(torch.float32)\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            # Concate skip with upsampled along channels dimension\n",
    "            x = torch.cat((x, skip), 1)\n",
    "\n",
    "\n",
    "        x = self.last(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # return torch.nn.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "inputs = torch.tensor(np.zeros((2,3,256,256)))\n",
    "np_im = np.transpose(generator(inputs)[0].detach().numpy(), (1,2,0)) / 255\n",
    "plt.imshow(np_im)\n",
    "LAMBDA = 10\n",
    "\n",
    "loss_object = torch.nn.BCELoss()\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(disc_generated_output, torch.ones_like(disc_generated_output))\n",
    "\n",
    "    # mean absolute error\n",
    "    l1_loss = torch.mean(torch.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down1 = downsample(6, 64, 4, apply_batchnorm=False)\n",
    "        self.down2 = downsample(64, 128, 4)\n",
    "        self.down3 = downsample(128, 256, 4)\n",
    "\n",
    "        self.zero_pad1 = torch.nn.ZeroPad2d(1)\n",
    "        self.conv = torch.nn.Conv2d(256, 512, 4, stride=1, bias=False)\n",
    "\n",
    "        self.batchnorm = torch.nn.BatchNorm2d(512)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU()\n",
    "\n",
    "        self.zero_pad2 = torch.nn.ZeroPad2d(1)\n",
    "\n",
    "        self.last = torch.nn.Conv2d(512, 1, 4, stride=1, bias=False)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inp, tar):\n",
    "        x = torch.cat((inp, tar), 1)\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "\n",
    "        x = self.zero_pad1(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leaky_relu(x)\n",
    "\n",
    "        x = self.zero_pad2(x)\n",
    "\n",
    "        x = self.last(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "discriminator = Discriminator()\n",
    "    \n",
    "# Define discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(disc_real_output, torch.ones_like(disc_real_output))\n",
    "\n",
    "    generated_loss = loss_object(disc_generated_output, torch.zeros_like(disc_generated_output))\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_step(input_images, targets, batch_size=2):\n",
    "\n",
    "\n",
    "\n",
    "    input_batch = input_images\n",
    "    target_batch = targets \n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        print(input_batch.shape)\n",
    "        gen_output = generator(input_batch)\n",
    "\n",
    "        disc_real_output = discriminator(input_batch, target_batch)\n",
    "        disc_generated_output = discriminator(input_batch, gen_output)\n",
    "\n",
    "        gen_loss, gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target_batch)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        gen_output = generator(input_batch)\n",
    "\n",
    "        disc_real_output = discriminator(input_batch, target_batch)\n",
    "        disc_generated_output = discriminator(input_batch, gen_output)\n",
    "\n",
    "        gen_loss, gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target_batch)\n",
    "\n",
    "        generator_optimizer.zero_grad()\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # total_seen += batch_size\n",
    "\n",
    "        # total_gen_loss += gen_loss\n",
    "        # total_gan_loss += gan_loss\n",
    "        # total_l1_loss += l1_loss\n",
    "        # total_disc_loss += disc_loss\n",
    "\n",
    "        # avg_gen_loss = float(total_gen_loss / total_seen)\n",
    "        # avg_gan_loss = float(total_gan_loss / total_seen)\n",
    "        # avg_l1_loss = float(total_l1_loss / total_seen)\n",
    "        # avg_disc_loss = float(total_disc_loss / total_seen)\n",
    "\n",
    "        print(f\"\\r[gen_loss={gen_loss:.3f}\\t disc_loss: {disc_loss:.3f}\", end='')\n",
    "    \n",
    "    return gen_loss, gan_loss, l1_loss, disc_loss\n",
    "\n",
    "def fit(train_ds, test_ds, epochs):\n",
    "    train_elevation_imgs, train_satellite_imgs = convert_ds_to_tensor(train_ds)\n",
    "    test_elevation_imgs, test_satellite_imgs = convert_ds_to_tensor(test_ds)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        train_step(train_elevation_imgs, train_satellite_imgs)\n",
    "        generate_images(generator, test_elevation_imgs, test_satellite_imgs)\n",
    "        if epoch % 5 + 1 == 0:\n",
    "            generate_images(generator, test_elevation_imgs, test_satellite_imgs)\n",
    "\n",
    "def convert_ds_to_tensor(ds):\n",
    "    elevation_imgs = []\n",
    "    satellite_imgs = []\n",
    "    for i in range(len(ds)):\n",
    "        elevation_imgs.append(ds[i]['elevation'])\n",
    "        satellite_imgs.append(ds[i]['satellite'])\n",
    "    \n",
    "    elevation_imgs = torch.stack(elevation_imgs)\n",
    "    satellite_imgs = torch.stack(satellite_imgs)\n",
    "\n",
    "    return elevation_imgs, satellite_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<-----------------Runnin----------------->\n",
      "\n",
      "Epoch 1/10\n",
      "torch.Size([288, 3, 256, 256])\n",
      "torch.Size([288, 3, 256, 256])\n",
      "torch.Size([288, 64, 128, 128])\n",
      "torch.Size([288, 128, 64, 64])\n",
      "torch.Size([288, 256, 32, 32])\n",
      "torch.Size([288, 512, 16, 16])\n",
      "torch.Size([288, 512, 8, 8])\n",
      "torch.Size([288, 512, 4, 4])\n",
      "torch.Size([288, 512, 2, 2])\n",
      "torch.Size([288, 6, 256, 256])\n",
      "torch.Size([288, 64, 128, 128])\n",
      "torch.Size([288, 128, 64, 64])\n",
      "torch.Size([288, 6, 256, 256])\n",
      "torch.Size([288, 64, 128, 128])\n",
      "torch.Size([288, 128, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m<-----------------Runnin----------------->\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m elevation_imgs, satellite_imgs \u001b[39m=\u001b[39m convert_ds_to_tensor(dataset)\n\u001b[0;32m----> 4\u001b[0m fit(dataset, dataset, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m generate_images(generator, test_elevation_imgs, test_satellite_imgs)\n",
      "Cell \u001b[0;32mIn[27], line 57\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_ds, test_ds, epochs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     train_step(train_elevation_imgs, train_satellite_imgs)\n\u001b[1;32m     58\u001b[0m     generate_images(generator, test_elevation_imgs, test_satellite_imgs)\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(input_images, targets, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m disc_real_output \u001b[39m=\u001b[39m discriminator(input_batch, target_batch)\n\u001b[1;32m     14\u001b[0m disc_generated_output \u001b[39m=\u001b[39m discriminator(input_batch, gen_output)\n\u001b[0;32m---> 16\u001b[0m gen_loss, gan_loss, l1_loss \u001b[39m=\u001b[39m generator_loss(disc_generated_output, gen_output, target_batch)\n\u001b[1;32m     17\u001b[0m disc_loss \u001b[39m=\u001b[39m discriminator_loss(disc_real_output, disc_generated_output)\n\u001b[1;32m     19\u001b[0m discriminator_optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[26], line 128\u001b[0m, in \u001b[0;36mgenerator_loss\u001b[0;34m(disc_generated_output, gen_output, target)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator_loss\u001b[39m(disc_generated_output, gen_output, target):\n\u001b[0;32m--> 128\u001b[0m     gan_loss \u001b[39m=\u001b[39m loss_object(disc_generated_output, torch\u001b[39m.\u001b[39;49mones_like(disc_generated_output))\n\u001b[1;32m    130\u001b[0m     \u001b[39m# mean absolute error\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     l1_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mabs(target \u001b[39m-\u001b[39m gen_output))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3096\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3098\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print('<-----------------Runnin----------------->')\n",
    "    elevation_imgs, satellite_imgs = convert_ds_to_tensor(dataset)\n",
    "    fit(dataset, dataset, 10)\n",
    "    generate_images(generator, test_elevation_imgs, test_satellite_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input)\n",
    "    print(\"generating images\")\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    i = np.random.randint(0, test_input.shape[0])\n",
    "\n",
    "    input_img = np.moveaxis(test_input[i].detach().numpy(), 0, 2)\n",
    "    ground_img = np.moveaxis(tar[i].detach().numpy(), 0, 2)\n",
    "    pred_img = np.moveaxis(prediction[i].detach().numpy(), 0, 2)\n",
    "\n",
    "    display_list = [input_img, ground_img, pred_img]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
