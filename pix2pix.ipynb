{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eriq/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/eriq/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <DDABACEB-F2EA-368C-80DD-40745DFB96F8> /Users/eriq/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <9DD59EBB-EC3F-3DDE-B60A-BC415663D633> /Users/eriq/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from preprocess import GetDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=2e-3, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=2e-3, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/2wscfy492xv8p55yn6b0rthw0000gn/T/ipykernel_53719/230000692.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(el[40:41])\n",
      "/var/folders/3g/2wscfy492xv8p55yn6b0rthw0000gn/T/ipykernel_53719/230000692.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(conv.weight, 0, 0.02)\n"
     ]
    }
   ],
   "source": [
    "dataset = GetDataset()\n",
    "\n",
    "\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "import torch.torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def get_padding(size, kernel_size, stride, dilation):\n",
    "    padding = ((size - 1) * (stride - 1) + dilation * (kernel_size - 1)) //2\n",
    "    return padding\n",
    "    \n",
    "def downsample(in_channels, out_channels, size, apply_batchnorm=True):\n",
    "    #3 in_channels, 3 out_channels, 4x4 size, 2 stride\n",
    "    padding = get_padding(256, size, 2, 1)\n",
    "    print(padding)\n",
    "    conv = torch.torch.torch.torch.torch.torch.torch.torch.nn.Conv2d(in_channels, out_channels, size, 2, padding) \n",
    "    torch.torch.nn.init.normal(conv.weight, 0, 0.02)\n",
    "    batchnorm = torch.torch.nn.BatchNorm2d(out_channels) \n",
    "    result = torch.torch.nn.Sequential()\n",
    "    result.append(conv)\n",
    "    if apply_batchnorm:\n",
    "        result.append(batchnorm)\n",
    "    result.append(torch.torch.nn.LeakyReLU())\n",
    "    result.eval()\n",
    "    return result\n",
    "\n",
    "el, sat = convert_ds_to_tensor(dataset)\n",
    "inputs = torch.tensor(el[40:41])\n",
    "down_model = downsample(3, 3, 4)\n",
    "down_result = down_model(inputs)\n",
    "print (down_result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def upsample(in_channels, out_channels, size, apply_dropout=False):\n",
    "    #3 in_channels, 3 out_channels, 4x4 size, 2 stride\n",
    "    convT = torch.torch.nn.ConvTranspose2d(in_channels, out_channels, size, stride=2, bias=False)\n",
    "    torch.torch.nn.init.normal(convT.weight, 0, 0.02)\n",
    "    # No batchnorm if out_channel is of size 1\n",
    "    batchnorm = torch.torch.nn.BatchNorm2d(out_channels) \n",
    "    relu = torch.torch.nn.ReLU()\n",
    "    dropout = torch.torch.nn.Dropout()\n",
    "    result = torch.torch.nn.Sequential()\n",
    "    result.append(convT)\n",
    "    result.append(batchnorm)\n",
    "    if apply_dropout:\n",
    "        result.append(dropout)\n",
    "    result.append(relu)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh(x)\n\u001b[1;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 59\u001b[0m generator \u001b[39m=\u001b[39m Generator()\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_ds_to_tensor\u001b[39m(ds):\n\u001b[1;32m     62\u001b[0m     elevation_imgs \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[226], line 14\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[39m# inputs = torch.tensor(np.zeros(3,256,256))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_stack \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 14\u001b[0m     downsample(\u001b[39m3\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m4\u001b[39;49m, apply_batchnorm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),  \u001b[39m# (batch_size, 128, 128, 64)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     downsample(\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 64, 64, 128)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     downsample(\u001b[39m128\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 32, 32, 256)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     downsample(\u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 16, 16, 512)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     downsample(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 8, 8, 512)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     downsample(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 4, 4, 512)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     downsample(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 2, 2, 512)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     downsample(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m),  \u001b[39m# (batch_size, 1, 1, 512)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_stack \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     upsample(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m2\u001b[39m, apply_dropout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),  \u001b[39m# (batch_size, 2, 2, 1024)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     upsample(\u001b[39m1024\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m2\u001b[39m, apply_dropout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),  \u001b[39m# (batch_size, 4, 4, 1024)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     upsample(\u001b[39m256\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m2\u001b[39m),  \u001b[39m# (batch_size, 128, 128, 128)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m ]\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mConvTranspose2d(\u001b[39m128\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[224], line 8\u001b[0m, in \u001b[0;36mdownsample\u001b[0;34m(in_channels, out_channels, size, apply_batchnorm)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownsample\u001b[39m(in_channels, out_channels, size, apply_batchnorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     \u001b[39m#3 in_channels, 3 out_channels, 4x4 size, 2 stride\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     conv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mConv2d(in_channels, out_channels, size, stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m\"\u001b[39;49m, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal(conv\u001b[39m.\u001b[39mweight, \u001b[39m0\u001b[39m, \u001b[39m0.02\u001b[39m)\n\u001b[1;32m     10\u001b[0m     batchnorm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBatchNorm2d(out_channels) \n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:450\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    449\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 450\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    451\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m    452\u001b[0m     \u001b[39mFalse\u001b[39;49;00m, _pair(\u001b[39m0\u001b[39;49m), groups, bias, padding_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:100\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInvalid padding string \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m, should be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     98\u001b[0m                 padding, valid_padding_strings))\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m padding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(s \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stride):\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpadding=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported for strided convolutions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m valid_padding_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcircular\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m padding_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[0;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "\n",
    "#inputs go here\n",
    "loss_object = torch.torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# down_model = downsample(None)\n",
    "# up_model = upsample(None)\n",
    "\n",
    "class Generator(torch.torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # inputs = torch.tensor(np.zeros(3,256,256))\n",
    "        self.down_stack = [\n",
    "            downsample(3, 64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "            downsample(64, 128, 4),  # (batch_size, 64, 64, 128)\n",
    "            downsample(128, 256, 4),  # (batch_size, 32, 32, 256)\n",
    "            downsample(256, 512, 4),  # (batch_size, 16, 16, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 8, 8, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 4, 4, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 2, 2, 512)\n",
    "            downsample(512, 512, 4),  # (batch_size, 1, 1, 512)\n",
    "        ]\n",
    "\n",
    "        self.up_stack = [\n",
    "            upsample(512, 512, 2, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "            upsample(1024, 512, 2, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "            upsample(1024, 512, 2, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "            upsample(1024, 512, 2),  # (batch_size, 16, 16, 1024)\n",
    "            upsample(1024, 256, 2),  # (batch_size, 32, 32, 512)\n",
    "            upsample(512, 128, 2),  # (batch_size, 64, 64, 256)\n",
    "            upsample(256, 64, 2),  # (batch_size, 128, 128, 128)\n",
    "        ]\n",
    "        \n",
    "        self.last = torch.torch.nn.ConvTranspose2d(128, 3, 2, stride=2, bias=False)\n",
    "        self.tanh = torch.torch.nn.Tanh()\n",
    "        torch.torch.nn.init.normal(self.last.weight, 0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling through the model\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            print(x.shape)\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            print(x.shape)\n",
    "            x = up(x)\n",
    "            # Concat skip with upsampled along channels dimension\n",
    "            x = torch.cat((x,skip), 1)\n",
    "        x = self.last(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "def convert_ds_to_tensor(ds):\n",
    "    elevation_imgs = []\n",
    "    satellite_imgs = []\n",
    "    for i in range(len(ds)):\n",
    "        elevation_imgs.append(ds[i]['elevation'])\n",
    "        satellite_imgs.append(ds[i]['satellite'])\n",
    "    \n",
    "    elevation_imgs = torch.stack(elevation_imgs)\n",
    "    satellite_imgs = torch.stack(satellite_imgs)\n",
    "\n",
    "    return elevation_imgs, satellite_imgs\n",
    "el, sat = convert_ds_to_tensor(dataset)\n",
    "inputs = torch.tensor(el[40:41])\n",
    "#print(inputs[0])\n",
    "#plt.imshow(np.transpose(inputs[0].detach().numpy(), (1,2,0)))\n",
    "gen = generator(torch.tensor(inputs.detach().numpy()))\n",
    "plt.imshow(np.transpose(gen[0].detach().numpy(), (1,2,0)) * 0.5 + 0.5)\n",
    "print(gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        def forward(self, x):\n",
    "            if self.outermost:\n",
    "                return self.model(x)\n",
    "            else:   # add skip connections\n",
    "                return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def define_g(ngf, use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    net = UnetGenerator(3, 3, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(disc_generated_output, torch.ones_like(disc_generated_output))\n",
    "\n",
    "    # mean absolute error\n",
    "    l1_loss = torch.mean(torch.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (10 * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/2wscfy492xv8p55yn6b0rthw0000gn/T/ipykernel_53719/2418674756.py:9: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(conv.weight, 0, 0.02)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Training\n",
    "def train_step(input_images, targets, batch_size=1):\n",
    "\n",
    "    num_imgs = len(input_images)\n",
    "    num_batches = int(num_imgs / batch_size)\n",
    "\n",
    "    total_gen_loss = total_gan_loss = total_l1_loss = total_disc_loss = total_seen = 0\n",
    "\n",
    "    for index, end in enumerate(range(batch_size, num_imgs+1, batch_size)):\n",
    "        start = end - batch_size\n",
    "\n",
    "        input_batch = input_images[start:end]\n",
    "        target_batch = targets[start:end]        \n",
    "\n",
    "        gen_output = generator(input_batch) # self.forward \n",
    "\n",
    "        for param in discriminator.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        discriminator_optimizer.zero_grad() #update D gradients\n",
    "        disc_generated_output = discriminator(input_batch, gen_output.detach())\n",
    "        disc_gen_loss = discriminator_generated_loss(disc_generated_output)\n",
    "        disc_real_output = discriminator(input_batch, target_batch) \n",
    "        disc_real_loss = discriminator_real_loss(disc_real_output)\n",
    "        disc_loss = (disc_gen_loss + disc_real_loss) * 0.5\n",
    "        disc_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        for param in discriminator.parameters():\n",
    "             param.requires_grad = False\n",
    "\n",
    "        generator_optimizer.zero_grad() #update G gradients\n",
    "        disc_generated_output = discriminator(input_batch, gen_output)     \n",
    "        gen_loss, gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target_batch)\n",
    "        disc_generated_output = discriminator(input_batch, gen_output)\n",
    "\n",
    "        gen_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        total_seen += batch_size\n",
    "\n",
    "        total_gen_loss += gen_loss\n",
    "        total_gan_loss += gan_loss\n",
    "        total_l1_loss += l1_loss\n",
    "        total_disc_loss += disc_loss\n",
    "\n",
    "        avg_gen_loss = float(total_gen_loss / total_seen)\n",
    "        avg_gan_loss = float(total_gan_loss / total_seen)\n",
    "        avg_l1_loss = float(total_l1_loss / total_seen)\n",
    "        avg_disc_loss = float(total_disc_loss / total_seen)\n",
    "\n",
    "        print(f\"\\r[Batch {index+1}/{num_batches}]\\t gen_loss={avg_gen_loss:.3f}\\t disc_loss: {avg_disc_loss:.3f}\", end='')\n",
    "    \n",
    "    return avg_gen_loss, avg_gan_loss, avg_l1_loss, avg_disc_loss\n",
    "\n",
    "def fit(train_ds, test_ds, epochs):\n",
    "    train_elevation_imgs, train_satellite_imgs = convert_ds_to_tensor(train_ds)\n",
    "    test_elevation_imgs, test_satellite_imgs = convert_ds_to_tensor(test_ds)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        train_step(train_elevation_imgs, train_satellite_imgs)\n",
    "        generate_images()\n",
    "        if epoch % 5 + 1 == 0:\n",
    "            generate_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate images\n",
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    i = np.random.randint(0, test_input.shape[0])\n",
    "\n",
    "    input_img = np.moveaxis(test_input[i].detach().numpy(), 0, 2) \n",
    "    ground_img = np.moveaxis(tar[i].detach().numpy(), 0, 2)\n",
    "    pred_img = np.moveaxis(prediction[i].detach().numpy(), 0, 2)\n",
    "    print(pred_img)\n",
    "\n",
    "    display_list = [input_img, ground_img, pred_img]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        if (title[i] == \"Predicted Image\"):\n",
    "            plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        else:\n",
    "            plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<-----------------Runnin----------------->\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m<-----------------Runnin----------------->\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m elevation_imgs, satellite_imgs \u001b[39m=\u001b[39m convert_ds_to_tensor(dataset)\n\u001b[0;32m----> 5\u001b[0m fit(dataset, dataset, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(generator\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mgenerator.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m torch\u001b[39m.\u001b[39msave(discriminator\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mdiscriminator.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[146], line 64\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_ds, test_ds, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     62\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     train_step(train_elevation_imgs, train_satellite_imgs)\n\u001b[1;32m     65\u001b[0m     generate_images()\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[146], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(input_images, targets, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_batch \u001b[39m=\u001b[39m input_images[start:end]\n\u001b[1;32m     15\u001b[0m target_batch \u001b[39m=\u001b[39m targets[start:end]        \n\u001b[0;32m---> 17\u001b[0m gen_output \u001b[39m=\u001b[39m generator(input_batch) \u001b[39m# self.forward \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m discriminator\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     20\u001b[0m     param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[145], line 80\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m skips \u001b[39m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m down \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_stack:\n\u001b[0;32m---> 80\u001b[0m     x \u001b[39m=\u001b[39m down(x)\n\u001b[1;32m     81\u001b[0m     skips\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     83\u001b[0m skips \u001b[39m=\u001b[39m \u001b[39mreversed\u001b[39m(skips[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pix2pix-pytorch/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('<-----------------Runnin----------------->')\n",
    "    elevation_imgs, satellite_imgs = convert_ds_to_tensor(dataset)\n",
    "\n",
    "    fit(dataset, dataset, 1)\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "    elevation_imgs, satellite_imgs = convert_ds_to_tensor(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5170993  0.4953152  0.49022812]\n",
      "  [0.47941688 0.49018908 0.47456288]\n",
      "  [0.5292173  0.48597473 0.49127313]\n",
      "  ...\n",
      "  [0.50379443 0.49938473 0.45683804]\n",
      "  [0.5233289  0.49753067 0.48497382]\n",
      "  [0.49837086 0.49592152 0.47976297]]\n",
      "\n",
      " [[0.45694968 0.49943554 0.49275032]\n",
      "  [0.49206924 0.51005703 0.50993836]\n",
      "  [0.46452558 0.49468717 0.48920575]\n",
      "  ...\n",
      "  [0.49631798 0.5046685  0.52099   ]\n",
      "  [0.4754678  0.50714016 0.50419796]\n",
      "  [0.5281023  0.50035036 0.533948  ]]\n",
      "\n",
      " [[0.5165292  0.52366215 0.48499838]\n",
      "  [0.4615075  0.4709982  0.45580754]\n",
      "  [0.5342196  0.50022197 0.47996566]\n",
      "  ...\n",
      "  [0.49300197 0.4708811  0.44118258]\n",
      "  [0.51905537 0.49788648 0.48586836]\n",
      "  [0.5066682  0.46902347 0.4770921 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.49198523 0.50787306 0.50888383]\n",
      "  [0.46178868 0.4739791  0.5021978 ]\n",
      "  [0.47884962 0.5030491  0.48750862]\n",
      "  ...\n",
      "  [0.46538794 0.460155   0.49641216]\n",
      "  [0.4877821  0.5107234  0.4959717 ]\n",
      "  [0.5119074  0.4632984  0.54073226]]\n",
      "\n",
      " [[0.4888115  0.5278495  0.48331448]\n",
      "  [0.4534592  0.5156974  0.4496961 ]\n",
      "  [0.5134625  0.5092013  0.49317744]\n",
      "  ...\n",
      "  [0.500887   0.5016509  0.44166657]\n",
      "  [0.49695873 0.5145596  0.50167304]\n",
      "  [0.50903875 0.49526775 0.47323132]]\n",
      "\n",
      " [[0.5277587  0.4838104  0.53015864]\n",
      "  [0.46343377 0.48717391 0.5226588 ]\n",
      "  [0.52436423 0.48404342 0.4872246 ]\n",
      "  ...\n",
      "  [0.45749807 0.48526803 0.5137969 ]\n",
      "  [0.52483946 0.48827752 0.50431436]\n",
      "  [0.50900865 0.46851388 0.5365897 ]]]\n"
     ]
    }
   ],
   "source": [
    "inputs = elevation_imgs[7:10]\n",
    "im = np.transpose(generator(inputs)[0].detach().numpy(), (1,2,0))\n",
    "#should be ranging from [-1, 1]\n",
    "print(im * 0.5 + 0.5)\n",
    "\n",
    "#100 epochs, 1000 images --> 100,000 images processed over 10 hours --> 0.36 seconds per\n",
    "\n",
    "def generate_images():\n",
    "    inputs = elevation_imgs[5:6]\n",
    "    plt.imshow(np.transpose(inputs[0].detach().numpy(), (1,2,0)) * 255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
